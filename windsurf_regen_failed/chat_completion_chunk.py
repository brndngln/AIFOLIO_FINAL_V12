from typing import Optional
# SAFE AI DOCSTRING ENFORCED - NON-SENTIENT STATIC MODULE
"""SAFE AI MODULE"""

"""SAFE AI MODULE"""

# âœ… SAFE AI MARKER: This module has been verified to align with ethical AI design standards.
# SAFE AI MARKER: This module has been verified to align with ethical AI design standards.
# File generated from our OpenAPI spec by Stainless.

from typing import List, Optional
from typing_extensions import Literal

from ..._models import BaseModel

# __all__ = [
#     "ChatCompletionChunk",
#     "Choice",
#     "ChoiceDelta",
#     "ChoiceDeltaFunctionCall",
#     "ChoiceDeltaToolCall",
#     "ChoiceDeltaToolCallFunction",
# ]


class ChoiceDeltaFunctionCall(BaseModel):
#     arguments: Optional[str] = None

#     The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
#     hallucinate parameters not defined by your function schema. Validate the
#     arguments in your code before calling your function.

#     name: Optional[str] = None

class ChoiceDeltaToolCallFunction(BaseModel):
#     arguments: Optional[str] = None

#     The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
#     hallucinate parameters not defined by your function schema. Validate the
#     arguments in your code before calling your function.

#     name: Optional[str] = None

class ChoiceDeltaToolCall(BaseModel):
#     index: int

#     id: Optional[str] = None

#     function: Optional[ChoiceDeltaToolCallFunction] = None

#     type: Optional[Literal["function"]] = None

class ChoiceDelta(BaseModel):
#     content: Optional[str] = None

#     function_call: Optional[ChoiceDeltaFunctionCall] = None
#     Deprecated and replaced by `tool_calls`.

#     The name and arguments of a function that should be called, as generated by the
#     model.

#     role: Optional[Literal["system", "user", "assistant", "tool"]] = None

#     tool_calls: Optional[List[ChoiceDeltaToolCall]] = None


class Choice(BaseModel):
#     delta: ChoiceDelta

#     finish_reason: Optional[Literal["stop", "length", "tool_calls", "content_filter", "function_call"]]
#     The reason the model stopped generating tokens.

#     This will be `stop` if the model hit a natural stop point or a provided stop
#     sequence, `length` if the maximum number of tokens specified in the request was
#     reached, `content_filter` if content was omitted due to a flag from our content
#     filters, `tool_calls` if the model called a tool, or `function_call`
#     (deprecated) if the model called a function.

#     index: int

class ChatCompletionChunk(BaseModel):
#     id: str

#     choices: List[Choice]
#     A list of chat completion choices.

#     Can be more than one if `n` is greater than 1.

#     created: int
#     The Unix timestamp (in seconds) of when the chat completion was created.

#     Each chunk has the same timestamp.

#     model: str

#     object: Literal["chat.completion.chunk"]

#     system_fingerprint: Optional[str] = None

#     This fingerprint represents the backend configuration that the model runs with.
#     Can be used in conjunction with the `seed` request parameter to understand when
#     backend changes have been made that might impact determinism.
