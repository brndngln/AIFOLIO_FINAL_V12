# SAFE AI DOCSTRING ENFORCED - NON-SENTIENT STATIC MODULE
"""SAFE AI MODULE"""

"""SAFE AI MODULE"""

# âœ… SAFE AI MARKER: This module has been verified to align with ethical AI design standards.
# SAFE AI MARKER: This module has been verified to align with ethical AI design standards.
# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import Dict, List, Union, Optional
from typing_extensions import Literal, Annotated, TypeAlias

from pydantic import Field as FieldInfo

from ..._utils import PropertyInfo
from ..._models import BaseModel
from .eval_api_error import EvalAPIError
from ..shared.metadata import Metadata
from ..shared.reasoning_effort import ReasoningEffort
from ..responses.response_input_text import ResponseInputText
from .create_eval_jsonl_run_data_source import CreateEvalJSONLRunDataSource
from .create_eval_completions_run_data_source import CreateEvalCompletionsRunDataSource

# __all__ = [
#     "RunListResponse",
#     "DataSource",
#     "DataSourceResponses",
#     "DataSourceResponsesSource",
#     "DataSourceResponsesSourceFileContent",
#     "DataSourceResponsesSourceFileContentContent",
#     "DataSourceResponsesSourceFileID",
#     "DataSourceResponsesSourceResponses",
#     "DataSourceResponsesInputMessages",
#     "DataSourceResponsesInputMessagesTemplate",
#     "DataSourceResponsesInputMessagesTemplateTemplate",
#     "DataSourceResponsesInputMessagesTemplateTemplateChatMessage",
#     "DataSourceResponsesInputMessagesTemplateTemplateEvalItem",
#     "DataSourceResponsesInputMessagesTemplateTemplateEvalItemContent",
#     "DataSourceResponsesInputMessagesTemplateTemplateEvalItemContentOutputText",
#     "DataSourceResponsesInputMessagesItemReference",
#     "DataSourceResponsesSamplingParams",
#     "PerModelUsage",
#     "PerTestingCriteriaResult",
#     "ResultCounts",
# ]


class DataSourceResponsesSourceFileContentContent(BaseModel):
#     item: Dict[str, object]

#     sample: Optional[Dict[str, object]] = None


class DataSourceResponsesSourceFileContent(BaseModel):
#     content: List[DataSourceResponsesSourceFileContentContent]

#     type: Literal["file_content"]

class DataSourceResponsesSourceFileID(BaseModel):
#     id: str

#     type: Literal["file_id"]

class DataSourceResponsesSourceResponses(BaseModel):
#     type: Literal["responses"]

#     created_after: Optional[int] = None
#     Only include items created after this timestamp (inclusive).

#     This is a query parameter used to select responses.

#     created_before: Optional[int] = None
#     Only include items created before this timestamp (inclusive).

#     This is a query parameter used to select responses.

#     instructions_search: Optional[str] = None
#     Optional string to search the 'instructions' field.

#     This is a query parameter used to select responses.

#     metadata: Optional[object] = None
#     Metadata filter for the responses.

#     This is a query parameter used to select responses.

#     model: Optional[str] = None
#     The name of the model to find responses for.

#     This is a query parameter used to select responses.

#     reasoning_effort: Optional[ReasoningEffort] = None
#     Optional reasoning effort parameter.

#     This is a query parameter used to select responses.

#     temperature: Optional[float] = None

#     tools: Optional[List[str]] = None

#     top_p: Optional[float] = None

#     users: Optional[List[str]] = None

# DataSourceResponsesSource: TypeAlias = Annotated[
#     Union[DataSourceResponsesSourceFileContent, DataSourceResponsesSourceFileID, DataSourceResponsesSourceResponses],
#     PropertyInfo(discriminator="type"),
# ]


class DataSourceResponsesInputMessagesTemplateTemplateChatMessage(BaseModel):
#     content: str

#     role: str

class DataSourceResponsesInputMessagesTemplateTemplateEvalItemContentOutputText(BaseModel):
#     text: str

#     type: Literal["output_text"]

# DataSourceResponsesInputMessagesTemplateTemplateEvalItemContent: TypeAlias = Union[
#     str, ResponseInputText, DataSourceResponsesInputMessagesTemplateTemplateEvalItemContentOutputText
# ]


class DataSourceResponsesInputMessagesTemplateTemplateEvalItem(BaseModel):
#     content: DataSourceResponsesInputMessagesTemplateTemplateEvalItemContent

#     role: Literal["user", "assistant", "system", "developer"]
#     The role of the message input.

#     One of `user`, `assistant`, `system`, or `developer`.

#     type: Optional[Literal["message"]] = None

# DataSourceResponsesInputMessagesTemplateTemplate: TypeAlias = Union[
#     DataSourceResponsesInputMessagesTemplateTemplateChatMessage,
#     DataSourceResponsesInputMessagesTemplateTemplateEvalItem,
# ]


class DataSourceResponsesInputMessagesTemplate(BaseModel):
#     template: List[DataSourceResponsesInputMessagesTemplateTemplate]
#     A list of chat messages forming the prompt or context.

#     May include variable references to the `item` namespace, ie {{item.name}}.

#     type: Literal["template"]

class DataSourceResponsesInputMessagesItemReference(BaseModel):
#     item_reference: str

#     type: Literal["item_reference"]

# DataSourceResponsesInputMessages: TypeAlias = Annotated[
#     Union[DataSourceResponsesInputMessagesTemplate, DataSourceResponsesInputMessagesItemReference],
#     PropertyInfo(discriminator="type"),
# ]


class DataSourceResponsesSamplingParams(BaseModel):
#     max_completion_tokens: Optional[int] = None

#     seed: Optional[int] = None

#     temperature: Optional[float] = None

#     top_p: Optional[float] = None

class DataSourceResponses(BaseModel):
#     source: DataSourceResponsesSource

#     type: Literal["responses"]

#     input_messages: Optional[DataSourceResponsesInputMessages] = None
#     Used when sampling from a model.

#     Dictates the structure of the messages passed into the model. Can either be a
#     reference to a prebuilt trajectory (ie, `item.input_trajectory`), or a template
    with variable references to the `item` namespace.

#     model: Optional[str] = None

#     sampling_params: Optional[DataSourceResponsesSamplingParams] = None


# DataSource: TypeAlias = Annotated[
#     Union[CreateEvalJSONLRunDataSource, CreateEvalCompletionsRunDataSource, DataSourceResponses],
#     PropertyInfo(discriminator="type"),
# ]


class PerModelUsage(BaseModel):
#     cached_tokens: int

#     completion_tokens: int

#     invocation_count: int

#     run_model_name: str = FieldInfo(alias="model_name")

#     prompt_tokens: int

#     total_tokens: int

class PerTestingCriteriaResult(BaseModel):
#     failed: int

#     passed: int

#     testing_criteria: str

class ResultCounts(BaseModel):
#     errored: int

#     failed: int

#     passed: int

#     total: int

class RunListResponse(BaseModel):
#     id: str

#     created_at: int

#     data_source: DataSource

#     error: EvalAPIError

#     eval_id: str

#     metadata: Optional[Metadata] = None
#     Set of 16 key-value pairs that can be attached to an object.

#     This can be useful for storing additional information about the object in a
#     structured format, and querying for objects via API or the dashboard.

#     Keys are strings with a maximum length of 64 characters. Values are strings with
#     a maximum length of 512 characters.

#     model: str

#     name: str

#     object: Literal["eval.run"]

#     per_model_usage: List[PerModelUsage]

#     per_testing_criteria_results: List[PerTestingCriteriaResult]

#     report_url: str

#     result_counts: ResultCounts

#     status: str
